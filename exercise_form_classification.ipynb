{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0cbfad",
   "metadata": {},
   "source": [
    "# Exercise Form Classification using 1D-CNN\n",
    "\n",
    "**Group Members:**\n",
    "- Lorenz Andalajao\n",
    "- Gabriel Diana\n",
    "- Ken Meiro Villareal\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771ef9a",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "This project implements a **Long Short-Term Memory (LSTM) neural network** to classify exercise form quality using wearable sensor data. The goal is to distinguish between correct biceps curl execution (Class A) and common incorrect variations (Classes B-E) using multivariate time-series data from accelerometer, gyroscope, and magnetometer sensors.\n",
    "\n",
    "### Dataset\n",
    "- **Source**: Weight Lifting Exercises Dataset (UCI Machine Learning Repository)\n",
    "- **Samples**: 4,024 segmented exercise repetitions\n",
    "- **Features**: 52 sensor features (gyroscope, accelerometer, magnetometer readings from belt, arm, forearm, and dumbbell)\n",
    "- **Classes**: 5 classes\n",
    "  - **Class A**: Correct execution\n",
    "  - **Class B**: Throwing elbows to the front\n",
    "  - **Class C**: Lifting dumbbell only halfway\n",
    "  - **Class D**: Lowering dumbbell only halfway\n",
    "  - **Class E**: Throwing hips to the front\n",
    "- **Format**: Segmented, multivariate time-series (each repetition is a sequence of sensor readings)\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "**Model Type**: LSTM-based Time-Series Classifier\n",
    "\n",
    "**Architecture Details**:\n",
    "```\n",
    "Input: [batch_size, sequence_length, num_features] (sequence of sensor readings)\n",
    "\n",
    "LSTM Layer:\n",
    "├── LSTM (input_size=num_features, hidden_size=128, num_layers=2, batch_first=True, dropout=0.5)\n",
    "\n",
    "Fully Connected Layers:\n",
    "├── Linear (128 → 64)\n",
    "├── ReLU\n",
    "├── Dropout (0.5)\n",
    "└── Linear (64 → 5)\n",
    "\n",
    "Output: [batch_size, 5] (class probabilities)\n",
    "```\n",
    "\n",
    "**Key Architecture Features**:\n",
    "- **LSTM Layer**: Captures temporal dependencies in sensor sequences\n",
    "- **Dropout**: Prevents overfitting\n",
    "- **Fully Connected Classifier**: Maps LSTM output to exercise class labels\n",
    "\n",
    "### Tools and Technologies\n",
    "\n",
    "**Deep Learning Framework**:\n",
    "- **PyTorch 2.9.1**: Neural network implementation, training, and inference\n",
    "- **torch.nn**: Model architecture components (LSTM, Linear, etc.)\n",
    "- **torch.optim**: Optimizers (Adam, SGD, RMSprop)\n",
    "- **torch.utils.data**: DataLoader for efficient batch processing\n",
    "\n",
    "**Data Processing**:\n",
    "- **NumPy**: Numerical operations and array manipulation\n",
    "- **Pandas**: Data loading, exploration, and preprocessing\n",
    "- **scikit-learn**: \n",
    "  - `LabelEncoder`: Target label encoding\n",
    "  - `train_test_split`: Data splitting (70% train, 15% validation, 15% test)\n",
    "  - `classification_report`: Performance metrics\n",
    "  - `confusion_matrix`: Error analysis\n",
    "\n",
    "**Visualization**:\n",
    "- **Matplotlib**: Training curves, loss/accuracy plots\n",
    "- **Seaborn**: Confusion matrices, statistical visualizations\n",
    "\n",
    "**Training Infrastructure**:\n",
    "- **Learning Rate Scheduler**: ReduceLROnPlateau for adaptive learning rate\n",
    "- **Early Stopping**: Based on validation accuracy\n",
    "- **Loss Function**: CrossEntropyLoss for multi-class classification\n",
    "- **Hardware**: CPU-based training (PyTorch CPU version)\n",
    "\n",
    "**Development Environment**:\n",
    "- **Jupyter Notebook**: Interactive development and documentation\n",
    "- **Python 3.x**: Programming language\n",
    "- **Device**: CPU (torch.device('cpu'))\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "**Data Split**:\n",
    "- Training: 70% (sequences)\n",
    "- Validation: 15% (sequences)\n",
    "- Test: 15% (sequences)\n",
    "- Stratified sampling to preserve class distribution\n",
    "\n",
    "**Training Parameters**:\n",
    "- **Epochs**: 50 (with early stopping)\n",
    "- **Loss Function**: CrossEntropyLoss\n",
    "- **Learning Rate Scheduler**: ReduceLROnPlateau (factor=0.5, patience=5)\n",
    "- **Batch Processing**: Mini-batch gradient descent\n",
    "\n",
    "**Preprocessing Pipeline**:\n",
    "1. Feature selection (52 sensor features)\n",
    "2. Missing value removal\n",
    "3. Grouping sensor readings into sequences per exercise repetition\n",
    "4. Label encoding (A→0, B→1, C→2, D→3, E→4)\n",
    "5. Train/validation/test split\n",
    "6. Sequence padding to uniform length\n",
    "7. Tensor conversion for PyTorch\n",
    "\n",
    "### Best Configuration\n",
    "\n",
    "**Optimal Hyperparameters**:\n",
    "- Model: LSTM\n",
    "- Hidden Size: 128\n",
    "- Layers: 2\n",
    "- Dropout: 0.5\n",
    "- Optimizer: Adam\n",
    "- Learning Rate: 0.001\n",
    "- Batch Size: 32\n",
    "\n",
    "**Performance**:\n",
    "- Test Accuracy: (to be determined)\n",
    "- Validation Accuracy: (to be determined)\n",
    "- Training Time: (to be determined)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51998d7d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9452295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM Model Definition ---\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMExerciseClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=5, dropout=0.5):\n",
    "\n",
    "        super(LSTMExerciseClassifier, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Use last time step output\n",
    "\n",
    "        out = lstm_out[:, -1, :]\n",
    "\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ea15f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dff621",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71eee0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4024, 159)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>raw_timestamp_part_1</th>\n",
       "      <th>raw_timestamp_part_2</th>\n",
       "      <th>cvtd_timestamp</th>\n",
       "      <th>new_window</th>\n",
       "      <th>num_window</th>\n",
       "      <th>roll_belt</th>\n",
       "      <th>pitch_belt</th>\n",
       "      <th>yaw_belt</th>\n",
       "      <th>total_accel_belt</th>\n",
       "      <th>...</th>\n",
       "      <th>gyros_forearm_x</th>\n",
       "      <th>gyros_forearm_y</th>\n",
       "      <th>gyros_forearm_z</th>\n",
       "      <th>accel_forearm_x</th>\n",
       "      <th>accel_forearm_y</th>\n",
       "      <th>accel_forearm_z</th>\n",
       "      <th>magnet_forearm_x</th>\n",
       "      <th>magnet_forearm_y</th>\n",
       "      <th>magnet_forearm_z</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eurico</td>\n",
       "      <td>1322489729</td>\n",
       "      <td>34670</td>\n",
       "      <td>28/11/2011 14:15</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>3.70</td>\n",
       "      <td>41.6</td>\n",
       "      <td>-82.8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-170</td>\n",
       "      <td>155</td>\n",
       "      <td>184</td>\n",
       "      <td>-1160</td>\n",
       "      <td>1400</td>\n",
       "      <td>-876</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eurico</td>\n",
       "      <td>1322489729</td>\n",
       "      <td>62641</td>\n",
       "      <td>28/11/2011 14:15</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>3.66</td>\n",
       "      <td>42.8</td>\n",
       "      <td>-82.5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-178</td>\n",
       "      <td>164</td>\n",
       "      <td>182</td>\n",
       "      <td>-1150</td>\n",
       "      <td>1410</td>\n",
       "      <td>-871</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eurico</td>\n",
       "      <td>1322489729</td>\n",
       "      <td>70653</td>\n",
       "      <td>28/11/2011 14:15</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>3.58</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-82.3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-182</td>\n",
       "      <td>172</td>\n",
       "      <td>185</td>\n",
       "      <td>-1130</td>\n",
       "      <td>1400</td>\n",
       "      <td>-863</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eurico</td>\n",
       "      <td>1322489729</td>\n",
       "      <td>82654</td>\n",
       "      <td>28/11/2011 14:15</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>3.56</td>\n",
       "      <td>44.4</td>\n",
       "      <td>-82.1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-185</td>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>-1120</td>\n",
       "      <td>1400</td>\n",
       "      <td>-855</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eurico</td>\n",
       "      <td>1322489729</td>\n",
       "      <td>90637</td>\n",
       "      <td>28/11/2011 14:15</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>3.57</td>\n",
       "      <td>45.1</td>\n",
       "      <td>-81.9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-188</td>\n",
       "      <td>195</td>\n",
       "      <td>188</td>\n",
       "      <td>-1100</td>\n",
       "      <td>1400</td>\n",
       "      <td>-843</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_name  raw_timestamp_part_1  raw_timestamp_part_2    cvtd_timestamp  \\\n",
       "0    eurico            1322489729                 34670  28/11/2011 14:15   \n",
       "1    eurico            1322489729                 62641  28/11/2011 14:15   \n",
       "2    eurico            1322489729                 70653  28/11/2011 14:15   \n",
       "3    eurico            1322489729                 82654  28/11/2011 14:15   \n",
       "4    eurico            1322489729                 90637  28/11/2011 14:15   \n",
       "\n",
       "  new_window  num_window  roll_belt  pitch_belt  yaw_belt  total_accel_belt  \\\n",
       "0         no           1       3.70        41.6     -82.8                 3   \n",
       "1         no           1       3.66        42.8     -82.5                 2   \n",
       "2         no           1       3.58        43.7     -82.3                 1   \n",
       "3         no           1       3.56        44.4     -82.1                 1   \n",
       "4         no           1       3.57        45.1     -81.9                 1   \n",
       "\n",
       "   ...  gyros_forearm_x gyros_forearm_y gyros_forearm_z  accel_forearm_x  \\\n",
       "0  ...            -0.05           -0.37           -0.43             -170   \n",
       "1  ...            -0.06           -0.37           -0.59             -178   \n",
       "2  ...            -0.05           -0.27           -0.72             -182   \n",
       "3  ...             0.02           -0.24           -0.79             -185   \n",
       "4  ...             0.08           -0.27           -0.82             -188   \n",
       "\n",
       "  accel_forearm_y accel_forearm_z  magnet_forearm_x  magnet_forearm_y  \\\n",
       "0             155             184             -1160              1400   \n",
       "1             164             182             -1150              1410   \n",
       "2             172             185             -1130              1400   \n",
       "3             182             188             -1120              1400   \n",
       "4             195             188             -1100              1400   \n",
       "\n",
       "   magnet_forearm_z  classe  \n",
       "0              -876       E  \n",
       "1              -871       E  \n",
       "2              -863       E  \n",
       "3              -855       E  \n",
       "4              -843       E  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'Example_WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv'\n",
    "df = pd.read_csv(data_path, skiprows=3)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a57b84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "Total columns with missing values: 100\n",
      "\n",
      "Columns with >50% missing values:\n",
      "kurtosis_roll_belt      3936\n",
      "kurtosis_picth_belt     3936\n",
      "kurtosis_yaw_belt       3936\n",
      "skewness_roll_belt      3936\n",
      "skewness_roll_belt.1    3936\n",
      "                        ... \n",
      "stddev_pitch_forearm    3936\n",
      "var_pitch_forearm       3936\n",
      "avg_yaw_forearm         3936\n",
      "stddev_yaw_forearm      3936\n",
      "var_yaw_forearm         3936\n",
      "Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"Total columns with missing values: {(missing_values > 0).sum()}\")\n",
    "print(f\"\\nColumns with >50% missing values:\")\n",
    "print(missing_values[missing_values > len(df) * 0.5].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ef56d",
   "metadata": {},
   "source": [
    "## 3A. Prepare Data for LSTM\n",
    "\n",
    "To use an LSTM, we need to organize the data so each sample is a sequence of sensor readings over time. We'll group by `user_name` and `num_window` to create sequences for each exercise repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b77593be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM Data Preparation ---\n",
    "# Group data by user and window to create sequences\n",
    "sensor_columns = [col for col in df.columns if 'gyros_' in col or 'accel_' in col or 'magnet_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1761131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_padded: (91, 61, 44)\n",
      "Length of y: 91\n",
      "Label encoding: {np.str_('A'): np.int64(0), np.str_('B'): np.int64(1), np.str_('C'): np.int64(2), np.str_('D'): np.int64(3), np.str_('E'): np.int64(4)}\n"
     ]
    }
   ],
   "source": [
    "# --- LSTM Sequence Grouping and Padding ---\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Group by user_name and num_window to get each exercise repetition as a sequence\n",
    "grouped = df.groupby(['user_name', 'num_window'])\n",
    "sequences = []\n",
    "labels = []\n",
    "for (user, window), group in grouped:\n",
    "    seq = group[sensor_columns].values\n",
    "    # Replace NaNs with column mean for each sequence\n",
    "    if np.isnan(seq).any():\n",
    "        col_means = np.nanmean(seq, axis=0)\n",
    "        inds = np.where(np.isnan(seq))\n",
    "        seq[inds] = np.take(col_means, inds[1])\n",
    "    # Replace any remaining NaNs (e.g., if col_means is NaN) with zero\n",
    "    seq[np.isnan(seq)] = 0.0\n",
    "    sequences.append(seq)\n",
    "    labels.append(group['classe'].iloc[0])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_seq_len = max([len(seq) for seq in sequences])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_seq_len, dtype='float32', padding='post', truncating='post')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "print(f\"Shape of X_padded: {X_padded.shape}\")\n",
    "print(f\"Length of y: {len(y)}\")\n",
    "print(f\"Label encoding: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "758b4b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before filtering: Counter({np.int64(4): 31, np.int64(0): 31, np.int64(1): 20, np.int64(3): 6, np.int64(2): 3})\n",
      "Class counts after filtering: Counter({np.int64(4): 31, np.int64(0): 31, np.int64(1): 20, np.int64(3): 6, np.int64(2): 3})\n"
     ]
    }
   ],
   "source": [
    "# --- Remove Underrepresented Classes for LSTM Splitting ---\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = Counter(y)\n",
    "print(\"Class counts before filtering:\", class_counts)\n",
    "\n",
    "# Find classes with at least 2 samples\n",
    "valid_classes = [cls for cls, count in class_counts.items() if count >= 2]\n",
    "\n",
    "# Filter sequences and labels\n",
    "valid_indices = [i for i, label in enumerate(y) if label in valid_classes]\n",
    "X_padded_filtered = X_padded[valid_indices]\n",
    "y_filtered = np.array([y[i] for i in valid_indices])\n",
    "\n",
    "class_counts_filtered = Counter(y_filtered)\n",
    "print(\"Class counts after filtering:\", class_counts_filtered)\n",
    "\n",
    "# Now use X_padded_filtered and y_filtered for train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dec8cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([63, 61, 44]), Val: torch.Size([13, 61, 44]), Test: torch.Size([14, 61, 44])\n"
     ]
    }
   ],
   "source": [
    "# --- LSTM Data Split and DataLoader Setup (robust to rare classes) ---\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# Split padded LSTM data (use filtered data to avoid ValueError)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_padded_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered)\n",
    "\n",
    "# Remove classes in y_temp with <2 samples before second split\n",
    "temp_class_counts = Counter(y_temp)\n",
    "valid_temp_classes = [cls for cls, count in temp_class_counts.items() if count >= 2]\n",
    "valid_temp_indices = [i for i, label in enumerate(y_temp) if label in valid_temp_classes]\n",
    "X_temp_valid = X_temp[valid_temp_indices]\n",
    "y_temp_valid = y_temp[valid_temp_indices]\n",
    "\n",
    "# Now split temp into val and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp_valid, y_temp_valid, test_size=0.5, random_state=42, stratify=y_temp_valid)\n",
    "\n",
    "# Impute any remaining NaNs in train/val/test splits\n",
    "for arr, name in zip([X_train, X_val, X_test], ['X_train', 'X_val', 'X_test']):\n",
    "    if np.isnan(arr).any():\n",
    "        col_means = np.nanmean(arr, axis=(0,1))\n",
    "        inds = np.where(np.isnan(arr))\n",
    "        arr[inds] = np.take(col_means, inds[2])\n",
    "    # Replace any remaining NaNs (e.g., if col_means is NaN) with zero\n",
    "    arr[np.isnan(arr)] = 0.0\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "# Force any remaining NaNs in tensors to zero\n",
    "X_train_tensor[torch.isnan(X_train_tensor)] = 0.0\n",
    "X_val_tensor[torch.isnan(X_val_tensor)] = 0.0\n",
    "X_test_tensor[torch.isnan(X_test_tensor)] = 0.0\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train: {X_train_tensor.shape}, Val: {X_val_tensor.shape}, Test: {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99a57c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]\n",
      "  Train Loss: 1.5745, Train Acc: 0.4286\n",
      "  Val Loss: 1.5709, Val Acc: 0.3846\n",
      "Epoch [10/50]\n",
      "  Train Loss: 1.5130, Train Acc: 0.5714\n",
      "  Val Loss: 1.4819, Val Acc: 0.6154\n",
      "Epoch [10/50]\n",
      "  Train Loss: 1.5130, Train Acc: 0.5714\n",
      "  Val Loss: 1.4819, Val Acc: 0.6154\n",
      "Epoch [15/50]\n",
      "  Train Loss: 1.2280, Train Acc: 0.6032\n",
      "  Val Loss: 1.1865, Val Acc: 0.5385\n",
      "Epoch [15/50]\n",
      "  Train Loss: 1.2280, Train Acc: 0.6032\n",
      "  Val Loss: 1.1865, Val Acc: 0.5385\n",
      "Epoch [20/50]\n",
      "  Train Loss: 0.9830, Train Acc: 0.6667\n",
      "  Val Loss: 0.8000, Val Acc: 0.6923\n",
      "Epoch [20/50]\n",
      "  Train Loss: 0.9830, Train Acc: 0.6667\n",
      "  Val Loss: 0.8000, Val Acc: 0.6923\n",
      "Epoch [25/50]\n",
      "  Train Loss: 0.7228, Train Acc: 0.7302\n",
      "  Val Loss: 0.7087, Val Acc: 0.6923\n",
      "Epoch [25/50]\n",
      "  Train Loss: 0.7228, Train Acc: 0.7302\n",
      "  Val Loss: 0.7087, Val Acc: 0.6923\n",
      "Epoch [30/50]\n",
      "  Train Loss: 0.6532, Train Acc: 0.7778\n",
      "  Val Loss: 0.6294, Val Acc: 0.7692\n",
      "Epoch [30/50]\n",
      "  Train Loss: 0.6532, Train Acc: 0.7778\n",
      "  Val Loss: 0.6294, Val Acc: 0.7692\n",
      "Epoch [35/50]\n",
      "  Train Loss: 0.5735, Train Acc: 0.8095\n",
      "  Val Loss: 0.4878, Val Acc: 0.8462\n",
      "Epoch [35/50]\n",
      "  Train Loss: 0.5735, Train Acc: 0.8095\n",
      "  Val Loss: 0.4878, Val Acc: 0.8462\n",
      "Epoch [40/50]\n",
      "  Train Loss: 0.5476, Train Acc: 0.8730\n",
      "  Val Loss: 0.5012, Val Acc: 0.8462\n",
      "Epoch [40/50]\n",
      "  Train Loss: 0.5476, Train Acc: 0.8730\n",
      "  Val Loss: 0.5012, Val Acc: 0.8462\n",
      "Epoch [45/50]\n",
      "  Train Loss: 0.4533, Train Acc: 0.8889\n",
      "  Val Loss: 0.4918, Val Acc: 0.8462\n",
      "Epoch [45/50]\n",
      "  Train Loss: 0.4533, Train Acc: 0.8889\n",
      "  Val Loss: 0.4918, Val Acc: 0.8462\n",
      "Epoch [50/50]\n",
      "  Train Loss: 0.4687, Train Acc: 0.8889\n",
      "  Val Loss: 0.4834, Val Acc: 0.8462\n",
      "Baseline LSTM Results:\n",
      "Validation Accuracy: 92.31%\n",
      "Test Accuracy: 78.57%\n",
      "Training Time: 4.87 seconds\n",
      "Epoch [50/50]\n",
      "  Train Loss: 0.4687, Train Acc: 0.8889\n",
      "  Val Loss: 0.4834, Val Acc: 0.8462\n",
      "Baseline LSTM Results:\n",
      "Validation Accuracy: 92.31%\n",
      "Test Accuracy: 78.57%\n",
      "Training Time: 4.87 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Run Baseline LSTM Experiment ---\n",
    "import time\n",
    "\n",
    "device = torch.device('cpu')\n",
    "input_size = X_train_tensor.shape[2]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = LSTMExerciseClassifier(input_size=input_size, hidden_size=128, num_layers=2, num_classes=num_classes, dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "num_epochs = 50\n",
    "start_time = time.time()\n",
    "model, history, best_val_acc = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer,\n",
    "    num_epochs=num_epochs, device=device, scheduler=scheduler, verbose=True\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_acc = best_val_acc\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Baseline LSTM Results:\")\n",
    "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47940277",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1e5b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM Training and Evaluation Functions ---\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, scheduler=None, verbose=True):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "        if verbose and (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    return model, history, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69895164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Evaluate model and generate predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # Get present classes in test set\n",
    "    present_classes = np.unique(all_labels)\n",
    "    if label_encoder is not None:\n",
    "        class_names = [label_encoder.inverse_transform([i])[0] for i in present_classes]\n",
    "    else:\n",
    "        class_names = [str(i) for i in present_classes]\n",
    "    report = classification_report(all_labels, all_preds, labels=present_classes, target_names=class_names, digits=4)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=present_classes)\n",
    "    return accuracy, report, cm, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874282c",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning Experiments\n",
    "\n",
    "We will systematically tune the following hyperparameters:\n",
    "1. **Learning Rate**: [0.001, 0.0001, 0.00001]\n",
    "2. **Batch Size**: [32, 64, 128]\n",
    "3. **Number of Filters**: [32, 64, 128]\n",
    "4. **Kernel Size**: [3, 5, 7]\n",
    "\n",
    "We'll start with a baseline configuration and then tune one parameter at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM experiment runner function defined!\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(config, experiment_name):\n",
    "    \"\"\"\n",
    "    Run a single training experiment with given configuration\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT: {experiment_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    # Initialize LSTM model\n",
    "    model = LSTMExerciseClassifier(\n",
    "        input_size=X_train_tensor.shape[2],\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=len(label_encoder.classes_),\n",
    "        dropout=config.get('dropout_rate', 0.5)\n",
    "    ).to(device)\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['learning_rate'])\n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    start_time = datetime.now()\n",
    "    model, history, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        num_epochs=config['num_epochs'], device=device, \n",
    "        scheduler=scheduler, verbose=True\n",
    "    )\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    # Evaluate on test set\n",
    "    test_acc, test_report, test_cm, _, _ = evaluate_model(model, test_loader, device, label_encoder=label_encoder)\n",
    "    result = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'config': config.copy(),\n",
    "        'history': history,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_report': test_report,\n",
    "        'test_cm': test_cm,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS: {experiment_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    print(f\"\\nClassification Report:\\n{test_report}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    # Plot training history\n",
    "    plot_training_history(history, title=experiment_name)\n",
    "    plot_confusion_matrix(test_cm, title=f'{experiment_name} - Confusion Matrix')\n",
    "    return result\n",
    "\n",
    "print(\"LSTM experiment runner function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d0952",
   "metadata": {},
   "source": [
    "### Experiment 1: Baseline Configuration (Adam Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc7c3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT: Baseline LSTM (Adam)\n",
      "================================================================================\n",
      "Configuration:\n",
      "  optimizer: Adam\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 64\n",
      "  num_epochs: 50\n",
      "  dropout_rate: 0.5\n",
      "================================================================================\n",
      "\n",
      "Epoch [5/50]\n",
      "  Train Loss: 1.5797, Train Acc: 0.3333\n",
      "  Val Loss: 1.5747, Val Acc: 0.3846\n",
      "Epoch [5/50]\n",
      "  Train Loss: 1.5797, Train Acc: 0.3333\n",
      "  Val Loss: 1.5747, Val Acc: 0.3846\n",
      "Epoch [10/50]\n",
      "  Train Loss: 1.5093, Train Acc: 0.3651\n",
      "  Val Loss: 1.4862, Val Acc: 0.3846\n",
      "Epoch [10/50]\n",
      "  Train Loss: 1.5093, Train Acc: 0.3651\n",
      "  Val Loss: 1.4862, Val Acc: 0.3846\n",
      "Epoch [15/50]\n",
      "  Train Loss: 1.3562, Train Acc: 0.3492\n",
      "  Val Loss: 1.2930, Val Acc: 0.5385\n",
      "Epoch [15/50]\n",
      "  Train Loss: 1.3562, Train Acc: 0.3492\n",
      "  Val Loss: 1.2930, Val Acc: 0.5385\n",
      "Epoch [20/50]\n",
      "  Train Loss: 1.1171, Train Acc: 0.6825\n",
      "  Val Loss: 1.1225, Val Acc: 0.6154\n",
      "Epoch [20/50]\n",
      "  Train Loss: 1.1171, Train Acc: 0.6825\n",
      "  Val Loss: 1.1225, Val Acc: 0.6154\n",
      "Epoch [25/50]\n",
      "  Train Loss: 0.7811, Train Acc: 0.7143\n",
      "  Val Loss: 0.7314, Val Acc: 0.6923\n",
      "Epoch [25/50]\n",
      "  Train Loss: 0.7811, Train Acc: 0.7143\n",
      "  Val Loss: 0.7314, Val Acc: 0.6923\n",
      "Epoch [30/50]\n",
      "  Train Loss: 0.5062, Train Acc: 0.7937\n",
      "  Val Loss: 0.4542, Val Acc: 0.8462\n",
      "Epoch [30/50]\n",
      "  Train Loss: 0.5062, Train Acc: 0.7937\n",
      "  Val Loss: 0.4542, Val Acc: 0.8462\n",
      "Epoch [35/50]\n",
      "  Train Loss: 0.4639, Train Acc: 0.8730\n",
      "  Val Loss: 0.5534, Val Acc: 0.8462\n",
      "Epoch [35/50]\n",
      "  Train Loss: 0.4639, Train Acc: 0.8730\n",
      "  Val Loss: 0.5534, Val Acc: 0.8462\n",
      "Epoch [40/50]\n",
      "  Train Loss: 0.2969, Train Acc: 0.9048\n",
      "  Val Loss: 0.5924, Val Acc: 0.9231\n",
      "Epoch [40/50]\n",
      "  Train Loss: 0.2969, Train Acc: 0.9048\n",
      "  Val Loss: 0.5924, Val Acc: 0.9231\n",
      "Epoch [45/50]\n",
      "  Train Loss: 0.2277, Train Acc: 0.9048\n",
      "  Val Loss: 0.5748, Val Acc: 0.9231\n",
      "Epoch [45/50]\n",
      "  Train Loss: 0.2277, Train Acc: 0.9048\n",
      "  Val Loss: 0.5748, Val Acc: 0.9231\n",
      "Epoch [50/50]\n",
      "  Train Loss: 0.2262, Train Acc: 0.9048\n",
      "  Val Loss: 0.4961, Val Acc: 0.9231\n",
      "Epoch [50/50]\n",
      "  Train Loss: 0.2262, Train Acc: 0.9048\n",
      "  Val Loss: 0.4961, Val Acc: 0.9231\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Baseline LSTM Experiment ---\u001b[39;00m\n\u001b[32m      3\u001b[39m baseline_config = {\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mAdam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result_baseline = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBaseline LSTM (Adam)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m experiment_results.append(result_baseline)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(config, experiment_name)\u001b[39m\n\u001b[32m     95\u001b[39m training_time = (datetime.now() - start_time).total_seconds()\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m test_acc, test_report, test_cm, _, _ = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m result = {\n\u001b[32m    104\u001b[39m \n\u001b[32m    105\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mexperiment_name\u001b[39m\u001b[33m'\u001b[39m: experiment_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m }\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, device, class_names)\u001b[39m\n\u001b[32m     27\u001b[39m         all_labels.extend(labels.numpy())\n\u001b[32m     29\u001b[39m accuracy = accuracy_score(all_labels, all_preds)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m report = \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m cm = confusion_matrix(all_labels, all_preds)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, report, cm, all_preds, all_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:2970\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2964\u001b[39m         warnings.warn(\n\u001b[32m   2965\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2966\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2967\u001b[39m             )\n\u001b[32m   2968\u001b[39m         )\n\u001b[32m   2969\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2974\u001b[39m         )\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2976\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# --- Baseline LSTM Experiment ---\n",
    "\n",
    "baseline_config = {\n",
    "\n",
    "    'optimizer': 'Adam',\n",
    "\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'batch_size': 64,\n",
    "\n",
    "    'num_epochs': 50,\n",
    "\n",
    "    'dropout_rate': 0.5\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "result_baseline = run_experiment(baseline_config, \"Baseline LSTM (Adam)\")\n",
    "\n",
    "experiment_results.append(result_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27441d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM Experiment: SGD Optimizer ---\n",
    "\n",
    "sgd_config = baseline_config.copy()\n",
    "\n",
    "sgd_config['optimizer'] = 'SGD'\n",
    "\n",
    "sgd_config['learning_rate'] = 0.01\n",
    "\n",
    "\n",
    "\n",
    "result_sgd = run_experiment(sgd_config, \"LSTM Optimizer: SGD\")\n",
    "\n",
    "experiment_results.append(result_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e989d",
   "metadata": {},
   "source": [
    "### Experiment 4: Tuning Learning Rate (Lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_low_config = baseline_config.copy()\n",
    "lr_low_config['learning_rate'] = 0.0001\n",
    "\n",
    "result_lr_low = run_experiment(lr_low_config, \"Learning Rate: 0.0001\")\n",
    "experiment_results.append(result_lr_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bf9e1",
   "metadata": {},
   "source": [
    "### Experiment 5: Tuning Learning Rate (Higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_high_config = baseline_config.copy()\n",
    "lr_high_config['learning_rate'] = 0.005\n",
    "\n",
    "result_lr_high = run_experiment(lr_high_config, \"Learning Rate: 0.005\")\n",
    "experiment_results.append(result_lr_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ef627",
   "metadata": {},
   "source": [
    "### Experiment 6: Tuning Batch Size (Smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a453c0",
   "metadata": {},
   "source": [
    "### Experiment 7: Tuning Batch Size (Larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cfbde",
   "metadata": {},
   "source": [
    "### Experiment 11: Tuning Kernel Size (Larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadadc2",
   "metadata": {},
   "source": [
    "## 7. Summary of All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM Experiment Summary Table ---\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for result in experiment_results:\n",
    "\n",
    "    summary_data.append({\n",
    "\n",
    "        'Experiment': result['experiment_name'],\n",
    "\n",
    "        'Optimizer': result['config']['optimizer'],\n",
    "\n",
    "        'Learning Rate': result['config']['learning_rate'],\n",
    "\n",
    "        'Batch Size': result['config']['batch_size'],\n",
    "\n",
    "        'Val Accuracy': f\"{result['best_val_acc']:.4f}\",\n",
    "\n",
    "        'Test Accuracy': f\"{result['test_acc']:.4f}\",\n",
    "\n",
    "        'Training Time (s)': f\"{result['training_time']:.2f}\"\n",
    "\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"SUMMARY OF LSTM EXPERIMENTS\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Find best configuration\n",
    "\n",
    "best_idx = summary_df['Test Accuracy'].astype(float).idxmax()\n",
    "\n",
    "print(f\"\\n🏆 BEST LSTM CONFIGURATION:\")\n",
    "\n",
    "print(summary_df.iloc[best_idx].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfdd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "test_accs = [float(r['test_acc']) for r in experiment_results]\n",
    "exp_names = [r['experiment_name'] for r in experiment_results]\n",
    "axes[0, 0].barh(exp_names, test_accs, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Validation Accuracy Comparison\n",
    "val_accs = [float(r['best_val_acc']) for r in experiment_results]\n",
    "val_accs = [float(r['best_val_acc']) for r in experiment_results]\n",
    "axes[0, 1].barh(exp_names, val_accs, color='coral')\n",
    "axes[0, 1].set_xlabel('Validation Accuracy')\n",
    "axes[0, 1].set_title('Validation Accuracy Comparison')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Training Time Comparison\n",
    "train_times = [r['training_time'] for r in experiment_results]\n",
    "axes[1, 0].barh(exp_names, train_times, color='mediumseagreen')\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)')\n",
    "axes[1, 0].set_title('Training Time Comparison')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Val vs Test Accuracy\n",
    "axes[1, 1].scatter(val_accs, test_accs, s=100, alpha=0.6, c=range(len(exp_names)), cmap='viridis')\n",
    "for i, name in enumerate(exp_names):\n",
    "    axes[1, 1].annotate(str(i+1), (val_accs[i], test_accs[i]), fontsize=8)\n",
    "axes[1, 1].plot([min(val_accs), max(val_accs)], [min(val_accs), max(val_accs)], 'r--', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Validation Accuracy')\n",
    "axes[1, 1].set_ylabel('Test Accuracy')\n",
    "axes[1, 1].set_title('Validation vs Test Accuracy')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9858b3",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Optimizer Comparison:**\n",
    "   - Compare the performance of Adam, SGD, and RMSprop optimizers\n",
    "   - Analyze convergence speed and final accuracy\n",
    "\n",
    "2. **Learning Rate Impact:**\n",
    "   - Effect of learning rate on training stability and convergence\n",
    "   - Optimal learning rate for the exercise classification task\n",
    "\n",
    "3. **Batch Size Impact:**\n",
    "   - Trade-off between batch size and training time\n",
    "   - Effect on generalization performance\n",
    "\n",
    "4. **Model Capacity:**\n",
    "   - Impact of number of filters on model performance\n",
    "   - Effect of kernel size on feature extraction\n",
    "\n",
    "5. **Best Configuration:**\n",
    "   - The configuration that achieved the highest test accuracy\n",
    "   - Recommended hyperparameters for production use\n",
    "\n",
    "### Project Requirements Met:\n",
    "- ✅ Trained neural network from scratch (no pretrained models)\n",
    "- ✅ Used multiple optimizers (Adam, SGD, RMSprop)\n",
    "- ✅ Performed systematic hyperparameter tuning\n",
    "- ✅ Recorded training configuration for each experiment\n",
    "- ✅ Documented training results (loss/accuracy curves)\n",
    "- ✅ Reported validation and test results\n",
    "- ✅ Generated confusion matrices for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f725d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results saved to 'experiment_results.json'\n",
      "\n",
      "Notebook execution complete! ✅\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "save_results = []\n",
    "for result in experiment_results:\n",
    "    save_results.append({\n",
    "        'experiment_name': result['experiment_name'],\n",
    "        'config': result['config'],\n",
    "        'best_val_acc': float(result['best_val_acc']),\n",
    "        'test_acc': float(result['test_acc']),\n",
    "        'training_time': float(result['training_time']),\n",
    "        'final_train_loss': float(result['history']['train_loss'][-1]),\n",
    "        'final_val_loss': float(result['history']['val_loss'][-1])\n",
    "    })\n",
    "\n",
    "with open('experiment_results.json', 'w') as f:\n",
    "    json.dump(save_results, f, indent=2)\n",
    "\n",
    "print(\"Experiment results saved to 'experiment_results.json'\")\n",
    "print(\"\\nNotebook execution complete! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking X_train_tensor:\n",
      "  Shape: torch.Size([63, 61, 44])\n",
      "  Contains NaN: False\n",
      "  Min value: -1160.0\n",
      "  Max value: 1440.0\n",
      "  Mean value: 21.319347381591797\n",
      "\n",
      "Checking X_val_tensor:\n",
      "  Shape: torch.Size([13, 61, 44])\n",
      "  Contains NaN: False\n",
      "  Min value: -725.0\n",
      "  Max value: 906.0\n",
      "  Mean value: 25.948427200317383\n",
      "\n",
      "Checking X_test_tensor:\n",
      "  Shape: torch.Size([14, 61, 44])\n",
      "  Contains NaN: False\n",
      "  Min value: -730.0\n",
      "  Max value: 1040.0\n",
      "  Mean value: 25.985321044921875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Data Integrity Check for LSTM Inputs ---\n",
    "import torch\n",
    "\n",
    "def check_tensor_integrity(tensor, name):\n",
    "    print(f\"Checking {name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Contains NaN: {torch.isnan(tensor).any().item()}\")\n",
    "    print(f\"  Min value: {tensor.min().item()}\")\n",
    "    print(f\"  Max value: {tensor.max().item()}\")\n",
    "    print(f\"  Mean value: {tensor.mean().item()}\")\n",
    "    print()\n",
    "\n",
    "check_tensor_integrity(X_train_tensor, 'X_train_tensor')\n",
    "check_tensor_integrity(X_val_tensor, 'X_val_tensor')\n",
    "check_tensor_integrity(X_test_tensor, 'X_test_tensor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (muffin-env)",
   "language": "python",
   "name": "muffin-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
